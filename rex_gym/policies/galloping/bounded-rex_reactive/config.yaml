!!python/object/new:rex_gym.agents.tools.attr_dict.AttrDict
dictitems:
  algorithm: !!python/name:rex_gym.agents.ppo.algorithm.PPOAlgorithm ''
  discount: 0.985
  env: !!python/object/apply:functools.partial
    args:
    - &id001 !!python/name:rex_gym.envs.gym.rex_reactive_env.RexReactiveEnv ''
    state: !!python/tuple
    - *id001
    - !!python/tuple []
    - accurate_motor_model_enabled: true
      control_latency: 0.02
      energy_weight: 0.005
      env_randomizer: null
      motor_kd: 0.015
      num_steps_to_log: 1000
      pd_latency: 0.003
      remove_default_joint_damping: true
      render: true
    - null
  eval_episodes: 25
  init_logstd: -1
  init_mean_factor: 0.05
  kl_cutoff_coef: 1000
  kl_cutoff_factor: 2
  kl_init_penalty: 1
  kl_target: 0.01
  logdir: /Users/seven/dev/test/galloping/bounded_action/20191109T031204-rex_reactive
  max_length: 1000
  network: !!python/name:rex_gym.agents.scripts.networks.ForwardGaussianPolicy ''
  num_agents: 14
  policy_layers: &id001 !!python/tuple
  - 200
  - 100
  policy_lr: 0.0001
  policy_optimizer: AdamOptimizer
  steps: 7000000.0
  update_epochs_policy: 50
  update_epochs_value: 50
  update_every: 25
  use_gpu: false
  value_layers: *id001
  value_lr: 0.0003
  value_optimizer: AdamOptimizer
  weight_summaries:
    all: .*
    policy: .*/policy/.*
    value: .*/value/.*
state:
  _mutable: false
